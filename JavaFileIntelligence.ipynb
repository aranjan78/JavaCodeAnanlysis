{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Java EE Project Analyzer with Groq API\n",
    "This notebook reads a Java EE project directory, processes files using LangChain, and analyzes them using Groq's model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSTRUCTIONS:<br>\n",
    "1. Quick Run of Java code analysis:<br>\n",
    "   - Fill PROJECT_PATH and GROQ_API_KEY in function run_with_predefined_paths() <br>\n",
    "   - **run_with_predefined_paths()** <br>\n",
    "2. The analysis may take several minutes for large projects due to API rate limiting.<br>\n",
    "3. Results will be saved to 'java_ee_analysis.json'<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: langchain in c:\\users\\mrash\\appdata\\roaming\\python\\python312\\site-packages (0.3.23)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\mrash\\appdata\\roaming\\python\\python312\\site-packages (0.3.21)\n",
      "Requirement already satisfied: langchain-groq in c:\\users\\mrash\\appdata\\roaming\\python\\python312\\site-packages (0.3.7)\n",
      "Requirement already satisfied: python-magic-bin in c:\\users\\mrash\\appdata\\roaming\\python\\python312\\site-packages (0.4.14)\n",
      "Requirement already satisfied: chardet in c:\\programdata\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: groq in c:\\users\\mrash\\appdata\\roaming\\python\\python312\\site-packages (0.31.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in c:\\users\\mrash\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.74)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\mrash\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\mrash\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (8.2.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.11.0)\n",
      "Requirement already satisfied: packaging>=23.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (24.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\mrash\\appdata\\roaming\\python\\python312\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.13)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\mrash\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\mrash\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (2.8.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\mrash\\appdata\\roaming\\python\\python312\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\mrash\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\mrash\\appdata\\roaming\\python\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from groq) (1.9.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-community langchain-groq python-magic-bin chardet groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import magic\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_groq import ChatGroq\n",
    "import chardet\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateLimiter:\n",
    "    def __init__(self, calls_per_second=2):\n",
    "        self.calls_per_second = calls_per_second\n",
    "        self.last_call_time = 0\n",
    "        \n",
    "    def wait_if_needed(self):\n",
    "        current_time = time.time()\n",
    "        time_since_last_call = current_time - self.last_call_time\n",
    "        min_interval = 1.0 / self.calls_per_second\n",
    "        \n",
    "        if time_since_last_call < min_interval:\n",
    "            time.sleep(min_interval - time_since_last_call)\n",
    "        \n",
    "        self.last_call_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JavaEEProjectAnalyzer:\n",
    "    def __init__(self, project_path: str, groq_api_key: str = None):\n",
    "        self.project_path = Path(project_path)\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "        )\n",
    "        \n",
    "        # Initialize Groq client\n",
    "        self.groq_api_key = groq_api_key or os.getenv('GROQ_API_KEY')\n",
    "        if not self.groq_api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable or pass it to constructor\")\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.groq_api_key,\n",
    "        #    model_name=\"deepseek-r1-distill-llama-70b\",\n",
    "            model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "            temperature=0.1,\n",
    "            max_tokens=4000\n",
    "        )\n",
    "        \n",
    "        self.rate_limiter = RateLimiter(calls_per_second=1.5)\n",
    "    \n",
    "    def detect_encoding(self, file_path: str) -> str:\n",
    "        \"\"\"Detect file encoding\"\"\"\n",
    "        with open(file_path, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "            result = chardet.detect(raw_data)\n",
    "            return result['encoding'] or 'utf-8'\n",
    "    \n",
    "    def read_java_files(self) -> List[Document]:\n",
    "        \"\"\"Read all Java files from the project directory\"\"\"\n",
    "        java_files = []\n",
    "        \n",
    "        for root, _, files in os.walk(self.project_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.java'):\n",
    "                    file_path = Path(root) / file\n",
    "                    try:\n",
    "                        encoding = self.detect_encoding(file_path)\n",
    "                        with open(file_path, 'r', encoding=encoding, errors='ignore') as f:\n",
    "                            content = f.read()\n",
    "                        \n",
    "                        java_files.append(Document(\n",
    "                            page_content=content,\n",
    "                            metadata={\n",
    "                                'file_path': str(file_path),\n",
    "                                'file_name': file,\n",
    "                                'relative_path': str(file_path.relative_to(self.project_path))\n",
    "                            }\n",
    "                        ))\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        return java_files\n",
    "    \n",
    "    def categorize_class(self, content: str, file_path: str) -> str:\n",
    "        \"\"\"Categorize Java class based on naming conventions and annotations\"\"\"\n",
    "        filename = Path(file_path).stem.lower()\n",
    "        \n",
    "        # Check for common annotations\n",
    "        annotations = {\n",
    "            '@Controller': 'Controller',\n",
    "            '@RestController': 'Controller',\n",
    "            '@Service': 'Service',\n",
    "            '@Repository': 'DAO',\n",
    "            '@Entity': 'Entity',\n",
    "            '@Component': 'Component',\n",
    "            '@Configuration': 'Configuration',\n",
    "            '@Bean': 'Configuration'\n",
    "        }\n",
    "        \n",
    "        for annotation, category in annotations.items():\n",
    "            if annotation in content:\n",
    "                return category\n",
    "        \n",
    "        # Check filename patterns\n",
    "        if any(pattern in filename for pattern in ['controller', 'ctrl']):\n",
    "            return 'Controller'\n",
    "        elif any(pattern in filename for pattern in ['service', 'impl']):\n",
    "            return 'Service'\n",
    "        elif any(pattern in filename for pattern in ['repository', 'dao', 'data']):\n",
    "            return 'DAO'\n",
    "        elif any(pattern in filename for pattern in ['entity', 'model', 'dto']):\n",
    "            return 'Entity'\n",
    "        elif any(pattern in filename for pattern in ['config', 'configuration']):\n",
    "            return 'Configuration'\n",
    "        elif any(pattern in filename for pattern in ['util', 'helper', 'utility']):\n",
    "            return 'Utility'\n",
    "        \n",
    "        return 'Other'\n",
    "    \n",
    "    def extract_class_info(self, content: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract class name and basic information\"\"\"\n",
    "        class_pattern = r'public\\s+(?:class|interface|enum|record)\\s+(\\w+)'\n",
    "        class_match = re.search(class_pattern, content)\n",
    "        class_name = class_match.group(1) if class_match else \"Unknown\"\n",
    "        \n",
    "        return {\n",
    "            'name': class_name,\n",
    "            'content': content\n",
    "        }\n",
    "    \n",
    "    def extract_methods(self, content: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extract methods from Java class content\"\"\"\n",
    "        methods = []\n",
    "        \n",
    "        # Pattern to match method signatures\n",
    "        method_pattern = r'(@?\\w+\\s+)*((public|private|protected)\\s+)?(\\w+(?:<\\w+>)?\\s+\\w+\\([^)]*\\))\\s*(?:throws\\s+\\w+(?:\\s*,\\s*\\w+)*)?\\s*\\{'\n",
    "        \n",
    "        matches = re.finditer(method_pattern, content, re.MULTILINE)\n",
    "        \n",
    "        for match in matches:\n",
    "            full_match = match.group(0)\n",
    "            method_signature = match.group(4)\n",
    "            \n",
    "            # Clean up the signature\n",
    "            method_signature = re.sub(r'\\s+', ' ', method_signature).strip()\n",
    "            \n",
    "            # Extract method name\n",
    "            method_name_match = re.search(r'(\\w+)\\(', method_signature)\n",
    "            method_name = method_name_match.group(1) if method_name_match else \"Unknown\"\n",
    "            \n",
    "            methods.append({\n",
    "                'name': method_name,\n",
    "                'signature': method_signature,\n",
    "                'description': '',\n",
    "                'complexity': ''\n",
    "            })\n",
    "        \n",
    "        return methods\n",
    "    \n",
    "    def analyze_with_llm_actual(self, chunks: List[Document]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze code chunks using Groq API with meta-llama/llama-4-maverick-17b-128e-instruct\"\"\"\n",
    "        result = {\n",
    "            \"projectOverview\": \"\",\n",
    "            \"Class\": []\n",
    "        }\n",
    "        \n",
    "        print(\"Getting project overview with LLM...\")\n",
    "        # Get project overview from first few chunks\n",
    "        overview_content = \"\\n\".join([chunk.page_content[:500] for chunk in chunks[:5]])\n",
    "        \n",
    "        overview_prompt = f\"\"\"\n",
    "        Analyze this Java EE project code and provide a comprehensive high-level overview of its business purpose, \n",
    "        main functionalities, and architecture. Be specific about what the project does.\n",
    "        \n",
    "        Sample code snippets:\n",
    "        {overview_content}\n",
    "        \n",
    "        Provide a concise but detailed project overview in 3-4 sentences.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            self.rate_limiter.wait_if_needed()\n",
    "            overview_response = self.llm.invoke(overview_prompt)\n",
    "            result[\"projectOverview\"] = overview_response.content.strip()\n",
    "            print(\"Project overview generated\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting project overview: {e}\")\n",
    "            result[\"projectOverview\"] = \"Java EE application with layered architecture\"\n",
    "        \n",
    "        print(\"Analyzing individual classes with LLM...\")\n",
    "        # Analyze each class\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"Analyzing class {i+1}/{len(chunks)}...\")\n",
    "            \n",
    "            class_info = self.extract_class_info(chunk.page_content)\n",
    "            category = self.categorize_class(chunk.page_content, chunk.metadata['file_path'])\n",
    "            methods = self.extract_methods(chunk.page_content)\n",
    "            \n",
    "            # Skip if no class name found or too many methods (likely not a class file)\n",
    "            if class_info['name'] == \"Unknown\" or len(methods) > 20:\n",
    "                continue\n",
    "            \n",
    "            # Prepare analysis prompt for this class\n",
    "            analysis_prompt = f\"\"\"\n",
    "            Analyze this Java class and provide detailed information in JSON format.\n",
    "            \n",
    "            Class Name: {class_info['name']}\n",
    "            Category: {category}\n",
    "            File: {chunk.metadata['file_name']}\n",
    "            \n",
    "            Class Content:\n",
    "            {chunk.page_content}\n",
    "            \n",
    "            Requirements:\n",
    "            1. Provide a comprehensive description of the class purpose and responsibilities\n",
    "            2. For each method in the class, provide:\n",
    "               - A detailed description of what the method does\n",
    "               - Complexity estimate (Low/Medium/High/Very High) based on logic, dependencies, and operations\n",
    "            3. Return ONLY valid JSON in this exact format:\n",
    "            {{\n",
    "                \"class_description\": \"detailed description here\",\n",
    "                \"methods\": [\n",
    "                    {{\n",
    "                        \"name\": \"methodName\",\n",
    "                        \"description\": \"detailed method description\",\n",
    "                        \"complexity\": \"Low/Medium/High/Very High\"\n",
    "                    }}\n",
    "                ]\n",
    "            }}\n",
    "            \n",
    "            Important: Return ONLY JSON, no additional text or explanations.\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                self.rate_limiter.wait_if_needed()\n",
    "                analysis_response = self.llm.invoke(analysis_prompt)\n",
    "                analysis_text = analysis_response.content.strip()\n",
    "                \n",
    "                # Try to extract JSON from the response\n",
    "                json_match = re.search(r'\\{.*\\}', analysis_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    analysis_data = json.loads(json_match.group())\n",
    "                    \n",
    "                    # Map the analyzed methods to our extracted methods\n",
    "                    analyzed_methods = []\n",
    "                    for extracted_method in methods:\n",
    "                        # Find matching analyzed method\n",
    "                        analyzed_method = next(\n",
    "                            (am for am in analysis_data.get('methods', []) \n",
    "                             if am['name'] == extracted_method['name']),\n",
    "                            None\n",
    "                        )\n",
    "                        \n",
    "                        if analyzed_method:\n",
    "                            analyzed_methods.append({\n",
    "                                \"name\": extracted_method['name'],\n",
    "                                \"signature\": extracted_method['signature'],\n",
    "                                \"description\": analyzed_method['description'],\n",
    "                                \"complexity\": analyzed_method['complexity']\n",
    "                            })\n",
    "                        else:\n",
    "                            # Fallback if LLM didn't analyze this method\n",
    "                            analyzed_methods.append({\n",
    "                                \"name\": extracted_method['name'],\n",
    "                                \"signature\": extracted_method['signature'],\n",
    "                                \"description\": f\"Method {extracted_method['name']} implementation\",\n",
    "                                \"complexity\": \"Medium\"\n",
    "                            })\n",
    "                    \n",
    "                    # Add class to result\n",
    "                    class_data = {\n",
    "                        \"name\": class_info['name'],\n",
    "                        \"Category\": category,\n",
    "                        \"description\": analysis_data.get('class_description', \n",
    "                                                       f\"{category} class {class_info['name']}\"),\n",
    "                        \"methods\": analyzed_methods\n",
    "                    }\n",
    "                    \n",
    "                    result[\"Class\"].append(class_data)\n",
    "                    print(f\"Analyzed {class_info['name']} ({category})\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"Could not parse JSON from LLM response for {class_info['name']}\")\n",
    "                    # Fallback for this class\n",
    "                    self.add_fallback_class(result, class_info, category, methods)\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON parsing error for {class_info['name']}: {e}\")\n",
    "                self.add_fallback_class(result, class_info, category, methods)\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing class {class_info['name']}: {e}\")\n",
    "                self.add_fallback_class(result, class_info, category, methods)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def add_fallback_class(self, result: Dict[str, Any], class_info: Dict[str, Any], \n",
    "                          category: str, methods: List[Dict[str, str]]):\n",
    "        \"\"\"Add fallback class data when LLM analysis fails\"\"\"\n",
    "        methods_with_fallback = []\n",
    "        for method in methods:\n",
    "            methods_with_fallback.append({\n",
    "                \"name\": method['name'],\n",
    "                \"signature\": method['signature'],\n",
    "                \"description\": f\"Method {method['name']} implementation\",\n",
    "                \"complexity\": \"Medium\"\n",
    "            })\n",
    "        \n",
    "        class_data = {\n",
    "            \"name\": class_info['name'],\n",
    "            \"Category\": category,\n",
    "            \"description\": f\"{category} class {class_info['name']}\",\n",
    "            \"methods\": methods_with_fallback\n",
    "        }\n",
    "        result[\"Class\"].append(class_data)\n",
    "        print(f\"âœ“ Added fallback for {class_info['name']}\")\n",
    "    \n",
    "    def analyze_project(self) -> Dict[str, Any]:\n",
    "        \"\"\"Main analysis method with Groq LLM\"\"\"\n",
    "        print(\"Reading Java files...\")\n",
    "        java_files = self.read_java_files()\n",
    "        \n",
    "        if not java_files:\n",
    "            raise ValueError(\"No Java files found in the specified directory\")\n",
    "        \n",
    "        print(f\"Found {len(java_files)} Java files\")\n",
    "        \n",
    "        print(\"Chunking files...\")\n",
    "        all_chunks = []\n",
    "        for doc in java_files:\n",
    "            chunks = self.text_splitter.split_documents([doc])\n",
    "            all_chunks.extend(chunks)\n",
    "        \n",
    "        print(f\"Created {len(all_chunks)} chunks\")\n",
    "        \n",
    "        print(\"Analyzing with Groq LLM...\")\n",
    "        analysis_result = self.analyze_with_llm_actual(all_chunks)\n",
    "        \n",
    "        return analysis_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_groq_connection(api_key: str) -> bool:\n",
    "    \"\"\"Test if Groq API is working\"\"\"\n",
    "    try:\n",
    "        llm = ChatGroq(\n",
    "            groq_api_key=api_key,\n",
    "#            model_name=\"deepseek-r1-distill-llama-70b\",\n",
    "             model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "\n",
    "            temperature=0.1\n",
    "        )\n",
    "        response = llm.invoke(\"Hello, are you working? Respond with 'OK' if ready.\")\n",
    "        print(\"âœ“ Groq connection successful!\")\n",
    "        print(f\"Response: {response.content}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Groq connection failed: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Configuration\n",
    "    PROJECT_PATH = input(\"Enter the path to your Java EE project: \").strip()\n",
    "    GROQ_API_KEY = input(\"Enter your Groq API key (or press Enter to use environment variable): \").strip()\n",
    "    \n",
    "    if not GROQ_API_KEY:\n",
    "        GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "    \n",
    "    if not GROQ_API_KEY:\n",
    "        print(\"Error: Groq API key is required\")\n",
    "        return\n",
    "    \n",
    "    # Test connection first\n",
    "    print(\"Testing Groq connection...\")\n",
    "    if not test_groq_connection(GROQ_API_KEY):\n",
    "        return\n",
    "    \n",
    "    # Initialize analyzer with Groq\n",
    "    analyzer = JavaEEProjectAnalyzer(PROJECT_PATH, GROQ_API_KEY)\n",
    "    \n",
    "    # Analyze project\n",
    "    try:\n",
    "        print(\"\\nStarting project analysis...\")\n",
    "        result = analyzer.analyze_project()\n",
    "        \n",
    "        # Save results to JSON file\n",
    "        output_file = \"java_ee_analysis_groq.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nAnalysis complete! Results saved to {output_file}\")\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nProject Overview: {result['projectOverview']}\")\n",
    "        print(f\"Total classes analyzed: {len(result['Class'])}\")\n",
    "        \n",
    "        # Count by category\n",
    "        categories = {}\n",
    "        for cls in result['Class']:\n",
    "            categories[cls['Category']] = categories.get(cls['Category'], 0) + 1\n",
    "        \n",
    "        print(\"\\nClasses by category:\")\n",
    "        for category, count in categories.items():\n",
    "            print(f\"  {category}: {count}\")\n",
    "        \n",
    "        # Show sample of results\n",
    "        if result['Class']:\n",
    "            print(f\"\\nSample class analysis:\")\n",
    "            sample_class = result['Class'][0]\n",
    "            print(f\"Class: {sample_class['name']} ({sample_class['Category']})\")\n",
    "            print(f\"Description: {sample_class['description']}\")\n",
    "            if sample_class['methods']:\n",
    "                sample_method = sample_class['methods'][0]\n",
    "                print(f\"Sample method: {sample_method['name']} - {sample_method['complexity']} complexity\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_predefined_paths():\n",
    "    \"\"\"Run analysis with predefined paths for testing\"\"\"\n",
    "    PROJECT_PATH = r\"C:\\Users\\mrash\\Downloads\\simple-springboot-app-master\"  # Change this\n",
    "    GROQ_API_KEY = \"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\"  # Change this or set environment variable\n",
    "    \n",
    "    if not os.path.exists(PROJECT_PATH):\n",
    "        print(f\"Project path does not exist: {PROJECT_PATH}\")\n",
    "        return\n",
    "    \n",
    "    analyzer = JavaEEProjectAnalyzer(PROJECT_PATH, GROQ_API_KEY)\n",
    "    \n",
    "    try:\n",
    "        result = analyzer.analyze_project()\n",
    "        \n",
    "        output_file = \"java_ee_analysis.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(result, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Analysis saved to {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Java files...\n",
      "Found 20 Java files\n",
      "Chunking files...\n",
      "Created 21 chunks\n",
      "Analyzing with Groq LLM...\n",
      "Getting project overview with LLM...\n",
      "Project overview generated\n",
      "Analyzing individual classes with LLM...\n",
      "Analyzing class 1/21...\n",
      "Analyzed StudentApplication (Other)\n",
      "Analyzing class 2/21...\n",
      "Analyzed StudentConfig (Configuration)\n",
      "Analyzing class 3/21...\n",
      "Analyzed AutowiredController (Controller)\n",
      "Analyzing class 4/21...\n",
      "Analyzing class 5/21...\n",
      "Analyzed ConfigurationPropertyController (Controller)\n",
      "Analyzing class 6/21...\n",
      "Analyzed StudentController (Controller)\n",
      "Analyzing class 7/21...\n",
      "Analyzed ValuePropertyController (Controller)\n",
      "Analyzing class 8/21...\n",
      "Analyzed InvalidFieldException (Other)\n",
      "Analyzing class 9/21...\n",
      "Analyzed InvalidHeaderFieldException (Other)\n",
      "Analyzing class 10/21...\n",
      "Analyzed StudentExceptionHandler (Controller)\n",
      "Analyzing class 11/21...\n",
      "Analyzed RequestHeaderInterceptor (Component)\n",
      "Analyzing class 12/21...\n",
      "Analyzed Student (Other)\n",
      "Analyzing class 13/21...\n",
      "Analyzing class 14/21...\n",
      "Analyzed ComplexProperty (Component)\n",
      "Analyzing class 15/21...\n",
      "Analyzed Property (Other)\n",
      "Analyzing class 16/21...\n",
      "Analyzed StudentRepository (DAO)\n",
      "Analyzing class 17/21...\n",
      "Analyzed Animal (Other)\n",
      "Analyzing class 18/21...\n",
      "Analyzed Cat (Service)\n",
      "Analyzing class 19/21...\n",
      "Analyzed Dog (Other)\n",
      "Analyzing class 20/21...\n",
      "Analyzed StudentService (Service)\n",
      "Analyzing class 21/21...\n",
      "Analyzed StudentApplicationTests (Other)\n",
      "Analysis saved to java_ee_analysis.json\n"
     ]
    }
   ],
   "source": [
    "run_with_predefined_paths()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
